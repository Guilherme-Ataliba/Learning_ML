# ðŸ”µ Regularized Linear Models

Regularization (i.e. constrain, reduce the degrees of freedom) is a good way to reduce overfitting. For example:
1. *Polynomial Regression*: Regularization is usually applied by reducing the number of polynomial degrees. 
2. *Linear Model*: Regularization is usually achieved by constraining the weights of the model.
	- Some typical algorithms include Ridge Regression, Lasso Regression, and Elastic Net, which are all different ways to constrain the weights.

#### Scale
Most regularized models are sensitive to scale, thus, before performing the regression (unless explicitly said so), you must scale the data.

#### Any Regression
As with linear models, any regularized linear model can be used to make a "any regression" (polynomial regression, for example) fit, using the technique mentioned in [[Any Regression - Polynomial Regression]].

# ðŸ”µ Ridge Regression
This is a regularized linear regression model. It works simply by adding a *regularization term* to the **cost function**, equal to
$$\text{regularization term} = \alpha \sum_{i=1}^n \theta_i^2$$
- This forces the algorithm not only to fit the data, but to also keep the model weight as small as possible. 
- Once the model is trained, you want to evaluate it by using unregularized performance measures. 

The parameter $\alpha$ controls how much you want to regularize the model (the weight of the regularization). 
- If $\alpha=0$ ridge regression reduces to linear regression.
- If $\alpha$ is very large, then all the weights end up very close to zero, and the result is a flat line going through the data's mean. You can think of it as if the algorithm tries to optimize the weights (minimum values possible), instead of the accuracy. 

The ridge regression cost function is then
$$J(\theta) = MSE(\theta) + \alpha\sum_{i=1}^n\theta_i^2$$
- Note that the bias term $\theta_0$ is not regularized, since the summation starts at one. That's why if $\alpha$ is too large, the algorithm converges to a flat line through the **mean**. 

If we define $\pmb{w}$ as the vector of feature weights ($\theta_0$ to $\theta_n$), then the regularization term is simply equal to 
$$\frac{1}{2}(||\pmb{w}||_2)^2$$
Where, $||\pmb{w}||_2$ is the $\mathcal{l}_2$ norm of the weight vector. 
- For gradient descent, you'd only add $\alpha w$ to the MSE gradient vector. 

## ðŸ”· Implementation
Ridge regression can be performed using the closed-form equation or by performing Gradient Descent.

#### Closed Form Solution
$\pmb{A}$ Is the (n+1) x (n+1) identity matrix, except with a 0 on the top-left cell, corresponding to the bias term. The closed form solution is then
$$\hat{\theta} = \left(\pmb{X}^T \pmb{X} + \alpha \pmb{A}\right)^{-1}\pmb{X}^T\pmb{y}$$

#### Scikit-Learn
```python
# The solver is a more optimized way of calculating the closed-form solution
ridge_reg = Ridge(alpha=1, solver="cholesky")
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])
```

```python
# Penalty adds a regularization term equal to the square of l2 norm of the weight vector 
# This is simply ridge regression
sgd_reg = SGDRegressor(penalty="l2")
sgd_reg.fit(X, y.ravel())
sgd_reg.predict([[1.5]])
```